{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries first\n",
    "import duckdb\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from pmdarima.arima import auto_arima\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "from scipy import stats\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get your MotherDuck token from env\n",
    "load_dotenv('.env')\n",
    "\n",
    "token = os.getenv('motherduck_token')\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to the database\n",
    "con = duckdb.connect(f\"md:?motherduck_token={token}\")\n",
    "\n",
    "# define your SQL query\n",
    "sql_query = \"SELECT * FROM stocks_clouddb.msft_data\"\n",
    "\n",
    "# execute the query and fetch the result into a DataFrame\n",
    "df = con.sql(\"SELECT * FROM stocks_clouddb.msft_data\").fetchdf().copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the first few rows of the dataset\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_close=df['close_price']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for Stationarity of Time Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(timeseries):\n",
    "    # determing rolling statistics \n",
    "    rolmean = timeseries.rolling(12).mean()\n",
    "    rolstd = timeseries.rolling(12).std()\n",
    "    # plot rolling statistics \n",
    "    plt.plot(timeseries, color='blue',label='Original')\n",
    "    plt.plot(rolmean, color='red', label='Rolling Mean')\n",
    "    plt.plot(rolstd, color='black',label ='Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean and Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "\n",
    "    print(\"Results of Dickey fuller test\") \n",
    "    adft= adfuller(timeseries, autolag ='AIC')\n",
    "    # output for dft won't define what the values are\n",
    "    # hence we manually write what values does it explain using a for loop\n",
    "    output = pd.Series(adft[0:4],index=['Test Statistics','p-value','No. of lags used','Number of observations used'])\n",
    "    for key,values in adft[4].items():\n",
    "        output['critical value (%s)'%key] =  values\n",
    "    print(output)\n",
    "test_stationarity(df_close)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to separate the trend and the seasonality from a time series\n",
    "# decompose the series\n",
    "result = seasonal_decompose(df_close, model='multiplicative', period = 30)\n",
    "fig = plt.figure()  \n",
    "fig = result.plot()  \n",
    "fig.set_size_inches(16, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if it's not stationary then eliminate trend\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 6\n",
    "df_log = np.log(df_close)\n",
    "moving_avg = df_log.rolling(12).mean()\n",
    "std_dev = df_log.rolling(12).std()\n",
    "plt.legend(loc='best')\n",
    "plt.title('Moving Average')\n",
    "plt.plot(std_dev, color =\"black\", label = \"Standard Deviation\")\n",
    "plt.plot(moving_avg, color=\"red\", label = \"Mean\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and training set and visualize it \n",
    "train_data, test_data = df_log[3:int(len(df_log)*0.9)], df_log[int(len(df_log)*0.9):]\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.grid(True)\n",
    "plt.xlabel('Dates')\n",
    "plt.ylabel('Closing Prices')\n",
    "plt.plot(df_log, 'green', label='Train data')\n",
    "plt.plot(test_data, 'blue', label='Test data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auto ARIMA Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_autoARIMA = auto_arima(train_data, start_p=0, start_q=0,\n",
    "                      test='adf',       # use adftest to find optimal 'd'\n",
    "                      max_p=3, max_q=3, # maximum p and q\n",
    "                      m=1,              # frequency of series\n",
    "                      d=None,           # let model determine 'd'\n",
    "                      seasonal=False,   # No Seasonality\n",
    "                      start_P=0, \n",
    "                      D=0, \n",
    "                      trace=True,\n",
    "                      error_action='ignore',  \n",
    "                      suppress_warnings=True, \n",
    "                      stepwise=True)\n",
    "print(model_autoARIMA.summary())\n",
    "model_autoARIMA.plot_diagnostics(figsize=(15,8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling\n",
    "model = ARIMA(train_data, order=(1,1,2))  \n",
    "fitted = model.fit()  \n",
    "print(fitted.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast\n",
    "fc = fitted.forecast(steps=len(test_data), alpha=0.05)  # Ensure fc has the same length as test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make as pandas series\n",
    "fc_series = pd.Series(fc, index=test_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the lower and upper bounds of the confidence interval manually\n",
    "forecast_std = fitted.get_prediction(start=0, end=len(train_data) - 1).se_mean\n",
    "alpha = 0.05  # 95% confidence interval\n",
    "z = stats.norm.ppf(1 - alpha / 2)\n",
    "lower_series = fc - z * forecast_std\n",
    "upper_series = fc + z * forecast_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_series = pd.Series(lower_series, index=test_data.index)\n",
    "upper_series = pd.Series(upper_series, index=test_data.index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "plt.figure(figsize=(10,5), dpi=100)\n",
    "plt.plot(train_data, label='training data')\n",
    "plt.plot(test_data, color = 'blue', label='Actual Stock Price')\n",
    "plt.plot(fc_series, color = 'orange',label='Predicted Stock Price')\n",
    "plt.fill_between(lower_series.index, lower_series, upper_series, \n",
    "                 color='k', alpha=.10)\n",
    "plt.title('MSFT Stock Price Prediction')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('MSFT Stock Price')\n",
    "plt.legend(loc='upper left', fontsize=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report performance\n",
    "mse = mean_squared_error(test_data, fc)\n",
    "print('MSE: '+str(mse))\n",
    "mae = mean_absolute_error(test_data, fc)\n",
    "print('MAE: '+str(mae))\n",
    "rmse = math.sqrt(mean_squared_error(test_data, fc))\n",
    "print('RMSE: '+str(rmse))\n",
    "mape = np.mean(np.abs(fc - test_data)/np.abs(test_data))\n",
    "print('MAPE: '+str(mape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn \n",
    "import keras\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout, LSTM\n",
    "from sklearn.model_selection import TimeSeriesSplit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.filter(['close_price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dataframe to a numpy array \n",
    "dataset = data.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of rows to train the model on \n",
    "training_data_len = math.ceil(len(dataset)*0.8)\n",
    "training_data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data \n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(dataset)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the scaled training dataset \n",
    "train_data = scaled_data[0:training_data_len, :]\n",
    "\n",
    "# split the data into x_train and y_train dataset \n",
    "X_train=[]\n",
    "y_train =[]\n",
    "\n",
    "for i in range(60, len(train_data)):\n",
    "    X_train.append(train_data[i-60:i,0])\n",
    "    y_train.append(train_data[i,0])\n",
    "    # set 60 as the time step \n",
    "    if i < 61: \n",
    "        print(X_train)\n",
    "        print(y_train)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the x_train and y_train to numpy arrays \n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the LSTM Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True , input_shape = (X_train.shape[1],1))) \n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model \n",
    "model.compile(optimizer='adam', loss='mse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model training \n",
    "history = model.fit(X_train, y_train, epochs =25, batch_size = 8, verbose =1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_per_epoch = history.history['loss']\n",
    "plt.plot(range(len(loss_per_epoch)),loss_per_epoch)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss per Epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the testing dataset \n",
    "# create a new array containing scaled values \n",
    "test_data = scaled_data[training_data_len -60:, :]\n",
    "\n",
    "# create the dataset x_test and y_test \n",
    "x_test =[]\n",
    "y_test = dataset[training_data_len:,:]\n",
    "\n",
    "for i in range(60, len(test_data)):\n",
    "    x_test.append(test_data[i-60:i,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the data to a numpy array\n",
    "x_test = np.array(x_test )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape the data \n",
    "x_test= np.reshape(x_test, (x_test.shape[0], x_test.shape[1],1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model predicted price values\n",
    "predictions = model.predict(x_test)\n",
    "predictions = scaler.inverse_transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the root mean squared error (RMSE)\n",
    "rmse= np.sqrt(np.mean(predictions -y_test)**2) \n",
    "rmse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data \n",
    "train = data[:training_data_len]\n",
    "valid = data[training_data_len:]\n",
    "valid['Predictions'] = predictions \n",
    "\n",
    "# visualize the data \n",
    "plt.figure (figsize = (16,8))\n",
    "plt.title('Model ')\n",
    "plt.xlabel('Data', fontsize =18)\n",
    "plt.ylabel('Close Price USD $', fontsize=18)\n",
    "\n",
    "plt.plot(train['close_price'])\n",
    "plt.plot(valid[['close_price', 'Predictions']])\n",
    "plt.legend(['Train','Valid','Predictions'], loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the valid and predicted prices \n",
    "valid "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stock-price-predition-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
